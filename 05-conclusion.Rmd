
# Conclusion

In this dissertation, we proposed an alternative framework for describing how survey estimates from nonprobability samples can affected by selection bias. In the absence of randomization, any statistical inferences are based on an implicit or explicit model that explains the relationship between a sample and the target population. When models are implicit, it is more likely that they have not been subject to scrutiny and are at a greater likelihood of being inappropriate. To date, the tools available to survey researchers have not offered a simple and coherent way to think about and analyze the assumptions that they make, consciously or not, when they make inferences from data with nonexistent or imperfect randomization. The framework proposed here, based on principles from causal inference, offers a simple checklist of the the three conditions that must be true for such inferences to be valid: exchangeability, positivity, and correct composition. 

In Chapter \@ref(ch2) we examined the theoretical similarities between causal inference and survey inference and showed how principles from the former can be applied fruitfully to the latter. We demonstrated the conceptual utility of the causal framework for thinking about selection bias in surveys and showed how it can be used to reason about why some methods and practices seem to work better than others. The goal of this Chapter was to describe these concepts in an accessible and nontechnical manner that can be readily understood by practitioners as well as methodologists and statisticians.

Chapter \@ref(ch3) went a step further and provided the mathematical details for how exchangeability, positivity, and composition bias affect survey estimates. The total bias can be decomposed into separate additive components associated with each error source. This permits researchers to target specific kinds of error and develop research methods that are focused on eliminating specific kinds of modeling error in much the same way that the TSE framework helps designers of probability-based surveys eliminate specific threats to randomization inference introduced at each stage of the data collection process. Moreover, these components can be estimated given the appropriate reference data. Although such data may be unavailable for many survey outcomes, this is also true for those trying to study coverage or nonresponse error in probability-based surveys. 

When such data are available, there are clear practical benefits. We were able to see with the civic engagement items that there were clear patterns with respect to the average level of exchangeability bias between samples despite high within-sample variability. It is easy to see how this analysis could be extended to explain not only differences between a nonprobability sample and the target population but also differences between samples from different sources. To date, studies comparing data quality from different samples have had little success in explaining why data quality is so variable [@gittelman2015; @kennedy2016; @yeager2011]. In such instances, researchers would not be limited to only those variables that are available on a reference sample but could use many different questions from parallel surveys to diagnose and explain differences between different nonprobability sample sources This sort of approach could also be used to identify instances where it might be fruitful to combine data from different sources and when to avoid doing so.

In Chapter \@ref(ch4) we compared the performance of four approaches to estimation under conditions of nonignorable selection. In Chapter \@ref(ch2) the civic engagement items were shown to suffer from high levels of confounding bias in general. In such instances, any model that assumes ignorability will be automatically misspecified. We saw that for this application, the doubly-robust outcome regression with residual bias correction (OR-RBC) generally performed best for bias, variance, and RMSE while outcome regression with a propensity score covariate (OR-PSC) performed worst on all three.  Likewise, there were clear similarities between OR-RBC and propensity weighting (PW) and between OR-PSC and outcome regression (OR). That these findings differ from those of @tan2018 suggest that studying the performance of various estimators when exchangeability and positivity assumptions are violated may be as important as studying their performance under ignorability. A fruitful avenue for future research in this vein would be to extend the analysis from Chapter \@ref(ch3) and derive the bias decompositions for these different kinds of estimators in order to better understand the conditions under which one approach or another should be preferred.


## Next steps

There are many directions in which research could proceed from here. Some of the most immediate would include the extensions described above as well as replications of these analyses using different sample sources, reference samples, and outcome variables. Re-analyses of earlier comparative studies of nonprobability samples could help uncover explanations for the variation in data quality that has been observed across sample sources. It would also be worthwhile to experiment with different types of machine learning procedures and try to find the most effective way to estimate these bias components.

We can also see how this framework could be applied in the development of a variety of diagnostic procedures. For instance, when reference data is available, it is possible to estimate $\deltahatexch$ for individual cases using BART or other machine learning methods. These values could then be analyzed using procedures such as classification and regression trees. These estimated values could also be treated as outcome variables. This would make it easy to see how $\deltahatexch$ is affected by different estimation procedures without necessarily having to derive a new formula. One can imagine that an analytic formulation for a method such as OR-PSC with BART, where estimates from one complex model are used as inputs to a second, could be difficult or impossible to derive, but applying OR-PSC to estimated values of $\deltahatexch$ would make evaluation straightforward.

In many instances, a reference sample with the covariate distribution will be available but not the outcome variable of interest. This prevents the estimation of these bias components, but approaches to sensitivity analysis such as the version proposed by @robins1999b --  in which a hypothetical confounding effect is added to the value of an outcome variable for each unit in a sample -- could prove powerful in conjunction with a propensity model to measure the robustness of results to different levels of unobserved confounding. 

More broadly, the 2013 AAPOR task force report called for both a framework and standard metrics that can used to evaluate the quality of estimates from nonprobability samples [@baker2013]. This dissertation has proposed a framework. Going forward it will be important to take the next step and create metrics that can be used as measures of data quality. While response and completion rates are not especially meaningful for many nonprobability surveys, it would certainly be possible to develop measures summarizing what share of the population is missing relative to a reference dataset. Another possibility would be to develop a family of indicators measuring balance relative to a reference sample -- much like the R indicators that have been developed for probability-based samples where auxiliary data is available for nonrespondents [@schouten2009; @schouten2011; @schouten2012].

One problem that has plagued public-opinion polling in particular is the inadequacy of the "margin of sampling error" as a measure of data quality. It may be that this framework could be employed, perhaps in conjunction with methods such as those developed by @manski2007 to calculate error bounds that incorporate both bias and variance based on estimated common support, compositional differences, and some reasonable assumptions about potential confounding. 

The fact that all these diagnostics are themselves model-based and involve their own sets of assumptions should not be considered problematic. As we have stated repeatedly, there is no escaping assumptions; evaluating our assumptions requires making further assumptions about assumptions. What is important is that the assumptions are reasonable, useful, and above all transparent.

## Revisiting Total Survey Error

We began this dissertation by outlining the ways in which the Total Survey Error (TSE) framework falls short as an approach for researching error in nonprobability survey samples and proposed the causal framework as a more appropriate alternative. While TSE attempts to isolate sources of error that results from defects in the sampling and data collection processes, the causal framework proposed here is focused on defects in the statistical model that is used to relate sample to population. From an inferential perspective, it makes sense for surveys that aspire to base inferences on randomization to prioritize the TSE framework. Likewise, for surveys where randomization plays no meaningful inferential role, it makes sense to focus on modeling assumptions and interrogating the manner in which those assumptions could be incorrect. In practice though, probability-based surveys suffer from undercoverage and nonresponse, forcing researchers to rely on statistical models and assumptions. Likewise, users of nonprobability surveys still need to worry about data collection. 

There are a variety of ways in which the two approaches can be complementary. The bias components and estimation procedures described in this dissertation are equally applicable to probability-based surveys. Given high rates of nonresponse, probability-based surveys have become more and more reliant upon models and statistical adjustment to correct for problems with coverage and nonresponse. Additional tools for evaluating these models can only help. 

Even though coverage and nonresponse do not hold the same sort of inferential significance for nonprobability samples as they do for probability-based samples, the use of recruitment strategies that appeal to a more diverse set of potential panelists or survey designs that are more likely to result in respondent participation are important both for efficiency and to ensure that survey designs produce samples that are consistent with the models used for estimation and do not introduce additional confounds. These are the types of problems where TSE's focus on the data collection process can be helpful no matter what inferential framework a survey uses.

In conclusion, it is our hope that this dissertation has raised more questions than it has answered. When it comes to studying error in nonprobability survey samples much of the difficulty has been figuring out the right questions to ask. The error framework and analyses included in this dissertation are only a first step. With luck, they will be further refined and built upon both in our own future work and in that of other statisticians, methodologists, and practitioners who can use the framework to improve the quality of their own research.
